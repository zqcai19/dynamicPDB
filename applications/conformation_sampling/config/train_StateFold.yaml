# Default or base configuration for SE(3) diffusion experiments.

defaults:
  - override hydra/launcher: joblib

data:
  ref_number: 1
  motion_number: 3
  frame_time: 1  # the lenght of time sequence
  frame_sample_step: 1 # random  select start index first and  split data with steps

  stage1: False
  time_norm: False
  step_split: null            # Split data into steps and randomly select a start index.
  fix_sample_start: null      # Sample starting from a fixed index (fix_sample_start).
  random_sample_train: False  # If True, samples are random; otherwise, sample windows do not overlap.
  is_extrapolation: False     # Enable extrapolation mode.
  split_percent: 0.7          # Percentage of data used for extrapolation.
  keep_first: null            # Keep the first N time sequences.
  dynamics: True
  eval_start_idx: 0           # start index for ref evaluation
  eval_end_idx: 10            # end index for ref evaluation
  csv_path: train_data.csv # train data path
  val_csv_path:  val_data.csv # val data path
  filtering:
    max_len: 200
  min_t: 0.01
  num_t: 100 # linespace for reverse diffuser for 0 to 1

diffuser:
  dynamics: ${data.dynamics}
  frame_time: ${data.frame_time}
  diffuse_trans: True
  diffuse_rot: True

  # R(3) diffuser arguments
  r3:
    min_b: 0.1
    max_b: 20.0
    coordinate_scaling: 0.1

  # SO(3) diffuser arguments
  so3:
    num_omega: 1000
    num_sigma: 1000
    min_sigma: 0.1
    max_sigma: 1.5
    schedule: logarithmic
    cache_dir: .cache/
    use_cached_score: False

model:
  cfg_drop_rate: 0.1
  cfg_drop_in_train: True
  cfg_gamma: 0.9 # from 1 to 7.5
  frame_time: ${data.frame_time}
  ref_number: ${data.ref_number}
  motion_number: ${data.motion_number}

  dirtect: False   # seq to temporal structure
  dynamics: ${data.dynamics}
  node_embed_size: 384
  edge_embed_size: 128
  dropout: 0.0
  embed:
    StateFold_embedder: True
    index_embed_size:  32  # 32 for scratch   64 for fine-tuning model
    aatype_embed_size: 32 # default 64
    embed_self_conditioning: False
    num_bins: 22
    min_bin: 1e-5
    max_bin: 20.0
    skip_feature: False
  ipa:
    c_s: ${model.node_embed_size}
    c_z: ${model.edge_embed_size}
    c_hidden: 256 # default 256
    c_skip: 64
    no_heads: 8 # default 8
    no_qk_points: 8  # default 8
    no_v_points: 12  # default 12
    seq_tfmr_num_heads: 4
    seq_tfmr_num_layers: 2 # should be 2 if load pretrained model from 30D_04M_2024Y_20h_32m_00s
    num_blocks: 4
    coordinate_scaling: ${diffuser.r3.coordinate_scaling}
    spatial: True
    temporal: True
    temporal_position_encoding: True
    temporal_position_max_len: 40
    frozen_spatial: False

experiment:
  # Experiment metadata
  name: StateFOLD_Temporal_${model.ipa.temporal}
  base_root: ../StateFOLD_res/result_v2_deep_debug
  run_id: null
  training: True

  #training mode
  use_ddp : True

  # Training arguments
  log_freq: 100
  batch_size: 4
  eval_batch_size: 1
  num_loader_workers: 12
  num_epoch: 300_000 #500
  learning_rate: 0.0001
  max_squared_res: 360000
  prefetch_factor: 100
  use_gpu: True
  num_gpus: 4
  # sample_mode: cluster_time_batch

  # tensorboard logging
  wandb_dir: ./
  use_wandb: False
  tensorboard_dir: ${experiment.base_root}/tensorboard/
  use_tensorboard: True

  # How many steps to checkpoint between.
  ckpt_freq: 1000
  # Take early checkpoint at step 100. Helpful for catching eval bugs early.
  early_ckpt: True


  ckpt_dir: ${experiment.base_root}/ckpt/
  use_warm_start_conf: False #True
  warm_start: null

  trans_loss_weight: 1.0
  rot_loss_weight: 1.0 #1.0
  rot_loss_t_threshold: 0.0
  separate_rot_loss: False
  trans_x0_threshold: 1.0
  coordinate_scaling: ${diffuser.r3.coordinate_scaling}
  bb_atom_loss_weight: 1.0
  bb_atom_loss_t_filter: 0.25
  dist_mat_loss_weight: 1.0 # 0.25
  dist_mat_loss_t_filter: 0.25
  aux_loss_weight: 0.25
  torsion_loss_weight: 1.0

  # validation dir.
  eval_dir: ${experiment.base_root}/val_outputs/
  noise_scale: 1.0
  # Filled in during training.
  num_parameters: null
  trainable_num_parameters: null

hydra:
  sweeper:
    params:
      # Example of hydra multi run and wandb.
      experiment.name: use_wandb
      experiment.use_wandb: True
  run:
    dir: ${experiment.base_root}/out_logs/${experiment.name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
